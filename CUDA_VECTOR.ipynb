{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPMV1k8G+odZlWSMl9my0+a",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lparis/Algo/blob/master/CUDA_VECTOR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yaZzjTLL0aUJ",
        "outputId": "f29b039b-4608-4b7e-81ab-5ac3b7f98cda"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting vector/vector_add.cu\n"
          ]
        }
      ],
      "source": [
        "%%writefile vector/vector_add.cu\n",
        "#include <iostream>  // Standard C++ header for input/output\n",
        "\n",
        "// Kernel - runs on GPU\n",
        "// __global__ marks it as callable from CPU, executed on GPU\n",
        "// threadIdx.x is a built-in CUDA variable - each thread gets a unique index\n",
        "__global__ void add(int *a, int *b, int *c) {\n",
        "    int i = threadIdx.x;\n",
        "    c[i] = a[i] + b[i];\n",
        "}\n",
        "\n",
        "// Standard C++ entry point, runs on CPU\n",
        "int main() {\n",
        "    // Host (CPU) memory - regular C++ arrays\n",
        "    int a[] = {1, 2, 3};\n",
        "    int b[] = {4, 5, 6};\n",
        "    int c[3] = {0, 0, 0};\n",
        "\n",
        "    // Allocate device (GPU) memory\n",
        "    // d_ prefix is convention meaning \"device\"\n",
        "    // cudaMalloc is like malloc but for GPU memory\n",
        "    int *d_a, *d_b, *d_c;\n",
        "    cudaMalloc(&d_a, 3 * sizeof(int));\n",
        "    cudaMalloc(&d_b, 3 * sizeof(int));\n",
        "    cudaMalloc(&d_c, 3 * sizeof(int));\n",
        "\n",
        "    // Copy data from CPU to GPU\n",
        "    cudaMemcpy(d_a, a, 3 * sizeof(int), cudaMemcpyHostToDevice);\n",
        "    cudaMemcpy(d_b, b, 3 * sizeof(int), cudaMemcpyHostToDevice);\n",
        "\n",
        "    // Launch kernel on the GPU\n",
        "    // <<<1, 3>>> means \"1 block, 3 threads per block\"\n",
        "    // 3 threads run in parallel, each handling one element\n",
        "    // GPU execution starts here\n",
        "    add<<<1, 3>>>(d_a, d_b, d_c);\n",
        "\n",
        "    // Wait for GPU to finish - kernel launches are asynchronous\n",
        "    cudaDeviceSynchronize();\n",
        "\n",
        "    // Copy results back from GPU to CPU\n",
        "    cudaMemcpy(c, d_c, 3 * sizeof(int), cudaMemcpyDeviceToHost);\n",
        "\n",
        "    // Print results - standard C++\n",
        "    std::cout << c[0] << \" \" << c[1] << \" \" << c[2] << std::endl;\n",
        "\n",
        "    // Free GPU memory\n",
        "    cudaFree(d_a); cudaFree(d_b); cudaFree(d_c);\n",
        "\n",
        "    return 0;\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc -arch=sm_75 vector/vector_add.cu -o vector/vector_add && ./vector/vector_add"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b5tkpExr2ZRl",
        "outputId": "63fde5a5-cd22-4764-cf4f-b25fc440ca36"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5 7 9\n"
          ]
        }
      ]
    }
  ]
}